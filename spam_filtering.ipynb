{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Filtering with Memory-Based and Naive Bayes models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook has been written as part of the project for the L101 module. It aims at studying Memory-Based and Naive Bayes models for Spam Filtering. It is composed of four parts:\n",
    "- Data pre-processing: multiple steps of pre-processing on the train set\n",
    "- Adaptation set pre-processing\n",
    "- Test set pre-processing\n",
    "- Naive Bayes models: apply Naive Bayes models to the data pre-processed previously  \n",
    "\n",
    "\n",
    "Memory-Based models are run thanks to TiMBL Software (Daelemans et al., 2000). To do so, we run in our console the command line: `timbl -f data_timbl/data_700.train -t data_timbl/data_700.test -wgr -dID -k1 +vcs`  \n",
    "This commmand line is composed of the following arguments:\n",
    "- `-f`: file with the train set in the C4.5 format (see Section 2.2 in the report)\n",
    "- `-t`: file with the adaptation/test set in the C4.5 format\n",
    "- `-w`: feature-weighting scheme (gr: Gain Ratio, 0: Equal Weights)\n",
    "- `-d`: distance-weighting scheme (z: Equal Distance, ID: Inverse Distance, IL: Inverse Linear, ED: Exponential Decay)\n",
    "- `+v`: output format (cs: class statistics with precision, recall, f1-score and AUC metrics)  \n",
    "\n",
    "\n",
    "Models and parameters selection are carried out with the train and adaptation sets, while methods comparison is realised with the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4wIKNkiGUVCn",
    "outputId": "8ad49b33-db83-475a-9588-dc5cf50b4631"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup \n",
    "import nltk\n",
    "import heapq\n",
    "from info_gain import info_gain\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1r_LRKIUVCw"
   },
   "source": [
    "### Data reading and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read XML files containing messages and transform them into a dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4M_3ARR1UVCx"
   },
   "outputs": [],
   "source": [
    "def xml_to_df(file_path):\n",
    "    file = open(file_path,\"rb\")\n",
    "    data = BeautifulSoup(file)\n",
    "    data = data.find_all('message')\n",
    "    df = pd.DataFrame()\n",
    "    for msg in data:\n",
    "        tag_dict = {}\n",
    "        for tag in msg.children:\n",
    "            if tag.name is not None:\n",
    "                if tag.text_normal is None:\n",
    "                    tag_dict[tag.name] = [tag.string]\n",
    "                else:\n",
    "                    tmp = tag.find_all(\"text_normal\")\n",
    "                    text_normal = \"\"\n",
    "                    for i in range(len(tmp)-1):\n",
    "                        tag_dict[tag.name + \"_normal_\" + str(i)] = tmp[i].get_text().replace(tmp[i+1].get_text(),'')\n",
    "                        text_normal += \"\\n\" + tag_dict[tag.name + \"_normal_\" + str(i)]\n",
    "                    tag_dict[tag.name + \"_normal_\" + str(len(tmp)-1)] = tmp[len(tmp)-1].get_text()\n",
    "                    text_normal += \"\\n\" + tag_dict[tag.name + \"_normal_\" + str(len(tmp)-1)]\n",
    "                    tag_dict[tag.name + \"_normal\"] = text_normal\n",
    "                if tag.text_embedded is not None:\n",
    "                    tmp = tag.find_all(\"text_embedded\")\n",
    "                    for i in range(len(tmp)-1):\n",
    "                        tag_dict[tag.name + \"_embedded_\" + str(i)] = tmp[i].get_text().replace(tmp[i+1].get_text(),'')\n",
    "                    tag_dict[tag.name + \"_embedded_\" + str(len(tmp)-1)] = tmp[len(tmp)-1].get_text()\n",
    "        df = df.append(pd.DataFrame(tag_dict))\n",
    "    df = df.drop(df.columns[df.notnull().sum() == 0], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AshPKTNiUVCy"
   },
   "outputs": [],
   "source": [
    "df_gen = xml_to_df(\"GenSpam/train_GEN.ems\")\n",
    "df_gen['spam'] = False\n",
    "df_spam = xml_to_df(\"GenSpam/train_SPAM.ems\")\n",
    "df_spam['spam'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jmoAdOxFUVCz"
   },
   "outputs": [],
   "source": [
    "df = df_gen.append(df_spam)\n",
    "df = df.sample(frac = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cptAP-SyUVCz"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>subject_normal_0</th>\n",
       "      <th>subject_normal</th>\n",
       "      <th>content-type</th>\n",
       "      <th>message_body_normal_0</th>\n",
       "      <th>message_body_normal</th>\n",
       "      <th>message_body_embedded_0</th>\n",
       "      <th>message_body_embedded_1</th>\n",
       "      <th>...</th>\n",
       "      <th>message_body_embedded_57</th>\n",
       "      <th>message_body_embedded_58</th>\n",
       "      <th>message_body_embedded_59</th>\n",
       "      <th>message_body_embedded_60</th>\n",
       "      <th>message_body_embedded_61</th>\n",
       "      <th>message_body_embedded_62</th>\n",
       "      <th>message_body_embedded_63</th>\n",
       "      <th>message_body_embedded_64</th>\n",
       "      <th>message_body_embedded_65</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fri, 04 Apr 2003 22:00:48 PST</td>\n",
       "      <td>\\n</td>\n",
       "      <td>org</td>\n",
       "      <td>\\n\\n^ Q-tips ( &amp;CHAR ) &amp;NAME &amp;NAME : A House ...</td>\n",
       "      <td>\\n \\n\\n^ Q-tips ( &amp;CHAR ) &amp;NAME &amp;NAME : A Hous...</td>\n",
       "      <td>text/html; charset=\"us-ascii\"</td>\n",
       "      <td>\\n\\n^ &amp;NAME &amp;NAME &amp;NAME - &amp;NAME &amp;NAME &amp;NAME A...</td>\n",
       "      <td>\\n \\n\\n^ &amp;NAME &amp;NAME &amp;NAME - &amp;NAME &amp;NAME &amp;NAME...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wed, 26 Mar 03 05:18:06 GMT</td>\n",
       "      <td>com</td>\n",
       "      <td>org</td>\n",
       "      <td>\\n\\n^ Re : online drug store , valium , viagr...</td>\n",
       "      <td>\\n \\n\\n^ Re : online drug store , valium , via...</td>\n",
       "      <td>multipart/alternative</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fri, 2 Jun 2000 13:31:19 +0100 (BST)</td>\n",
       "      <td>ac.uk</td>\n",
       "      <td>ac.uk</td>\n",
       "      <td>\\r\\n\\r\\n^ Re : &amp;NAME ! \\r\\n</td>\n",
       "      <td>\\n \\r\\n\\r\\n^ Re : &amp;NAME ! \\r\\n</td>\n",
       "      <td>TEXT/PLAIN; charset=US-ASCII</td>\n",
       "      <td>\\r\\n\\r\\n^ Watch for buses when crossing the r...</td>\n",
       "      <td>\\n \\r\\n\\r\\n^ Watch for buses when crossing the...</td>\n",
       "      <td>\\r\\n\\r\\n^ &amp;NAME ! ! ! ! ! \\r\\n^ It went reall...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wed, 30 Jan 2002 19:49:42 +0000</td>\n",
       "      <td>ac.uk</td>\n",
       "      <td>ac.uk</td>\n",
       "      <td>\\r\\n\\r\\n^ Re : Information \\r\\n</td>\n",
       "      <td>\\n \\r\\n\\r\\n^ Re : Information \\r\\n</td>\n",
       "      <td>text/plain; charset=us-ascii</td>\n",
       "      <td>\\r\\n\\r\\n^ PS - I like \" &amp;NAME \" ' if it 's al...</td>\n",
       "      <td>\\n \\r\\n\\r\\n^ PS - I like \" &amp;NAME \" ' if it 's ...</td>\n",
       "      <td>\\r\\n\\r\\n^ Dear &amp;NAME , \\r\\n^ &amp;NAME , I forgot...</td>\n",
       "      <td>\\r\\n\\r\\n^ Oh ! ! ! \\r\\n^ Good thing I checked...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wed, 9 Apr 2003 12:34:43 +0300</td>\n",
       "      <td>\\n</td>\n",
       "      <td>org</td>\n",
       "      <td>\\n\\n^ STOP &amp;NAME STARTING FROM TODAY ( &amp;NAME ...</td>\n",
       "      <td>\\n \\n\\n^ STOP &amp;NAME STARTING FROM TODAY ( &amp;NAM...</td>\n",
       "      <td>text/html; charset=ISO-8859-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     date     from       to  \\\n",
       "0          Fri, 04 Apr 2003 22:00:48 PST        \\n     org    \n",
       "1            Wed, 26 Mar 03 05:18:06 GMT      com      org    \n",
       "2   Fri, 2 Jun 2000 13:31:19 +0100 (BST)    ac.uk    ac.uk    \n",
       "3        Wed, 30 Jan 2002 19:49:42 +0000    ac.uk    ac.uk    \n",
       "4         Wed, 9 Apr 2003 12:34:43 +0300        \\n     org    \n",
       "\n",
       "                                    subject_normal_0  \\\n",
       "0   \\n\\n^ Q-tips ( &CHAR ) &NAME &NAME : A House ...   \n",
       "1   \\n\\n^ Re : online drug store , valium , viagr...   \n",
       "2                        \\r\\n\\r\\n^ Re : &NAME ! \\r\\n   \n",
       "3                    \\r\\n\\r\\n^ Re : Information \\r\\n   \n",
       "4   \\n\\n^ STOP &NAME STARTING FROM TODAY ( &NAME ...   \n",
       "\n",
       "                                      subject_normal  \\\n",
       "0  \\n \\n\\n^ Q-tips ( &CHAR ) &NAME &NAME : A Hous...   \n",
       "1  \\n \\n\\n^ Re : online drug store , valium , via...   \n",
       "2                     \\n \\r\\n\\r\\n^ Re : &NAME ! \\r\\n   \n",
       "3                 \\n \\r\\n\\r\\n^ Re : Information \\r\\n   \n",
       "4  \\n \\n\\n^ STOP &NAME STARTING FROM TODAY ( &NAM...   \n",
       "\n",
       "                      content-type  \\\n",
       "0   text/html; charset=\"us-ascii\"    \n",
       "1           multipart/alternative    \n",
       "2    TEXT/PLAIN; charset=US-ASCII    \n",
       "3    text/plain; charset=us-ascii    \n",
       "4   text/html; charset=ISO-8859-1    \n",
       "\n",
       "                               message_body_normal_0  \\\n",
       "0   \\n\\n^ &NAME &NAME &NAME - &NAME &NAME &NAME A...   \n",
       "1                                                NaN   \n",
       "2   \\r\\n\\r\\n^ Watch for buses when crossing the r...   \n",
       "3   \\r\\n\\r\\n^ PS - I like \" &NAME \" ' if it 's al...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                 message_body_normal  \\\n",
       "0  \\n \\n\\n^ &NAME &NAME &NAME - &NAME &NAME &NAME...   \n",
       "1                                                NaN   \n",
       "2  \\n \\r\\n\\r\\n^ Watch for buses when crossing the...   \n",
       "3  \\n \\r\\n\\r\\n^ PS - I like \" &NAME \" ' if it 's ...   \n",
       "4                                                NaN   \n",
       "\n",
       "                             message_body_embedded_0  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2   \\r\\n\\r\\n^ &NAME ! ! ! ! ! \\r\\n^ It went reall...   \n",
       "3   \\r\\n\\r\\n^ Dear &NAME , \\r\\n^ &NAME , I forgot...   \n",
       "4                                                NaN   \n",
       "\n",
       "                             message_body_embedded_1  ...  \\\n",
       "0                                                NaN  ...   \n",
       "1                                                NaN  ...   \n",
       "2                                                NaN  ...   \n",
       "3   \\r\\n\\r\\n^ Oh ! ! ! \\r\\n^ Good thing I checked...  ...   \n",
       "4                                                NaN  ...   \n",
       "\n",
       "  message_body_embedded_57 message_body_embedded_58 message_body_embedded_59  \\\n",
       "0                      NaN                      NaN                      NaN   \n",
       "1                      NaN                      NaN                      NaN   \n",
       "2                      NaN                      NaN                      NaN   \n",
       "3                      NaN                      NaN                      NaN   \n",
       "4                      NaN                      NaN                      NaN   \n",
       "\n",
       "  message_body_embedded_60 message_body_embedded_61 message_body_embedded_62  \\\n",
       "0                      NaN                      NaN                      NaN   \n",
       "1                      NaN                      NaN                      NaN   \n",
       "2                      NaN                      NaN                      NaN   \n",
       "3                      NaN                      NaN                      NaN   \n",
       "4                      NaN                      NaN                      NaN   \n",
       "\n",
       "  message_body_embedded_63 message_body_embedded_64 message_body_embedded_65  \\\n",
       "0                      NaN                      NaN                      NaN   \n",
       "1                      NaN                      NaN                      NaN   \n",
       "2                      NaN                      NaN                      NaN   \n",
       "3                      NaN                      NaN                      NaN   \n",
       "4                      NaN                      NaN                      NaN   \n",
       "\n",
       "    spam  \n",
       "0   True  \n",
       "1   True  \n",
       "2  False  \n",
       "3  False  \n",
       "4   True  \n",
       "\n",
       "[5 rows x 97 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.reset_index().drop(['index'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MXFEHj2qUVC4"
   },
   "outputs": [],
   "source": [
    "filter_col = [col for col in df if col.startswith('message_body') or col.startswith('subject_normal')]\n",
    "for col in filter_col:\n",
    "    df[col] = df[col].apply(lambda x: str(x).replace('\\n','').replace('\\r','').replace('^',''))\n",
    "df['to'] = df['to'].apply(lambda x: str(x).replace('\\n',''))\n",
    "df['from'] = df['from'].apply(lambda x: str(x).replace('\\n',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save pre-processed dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tvx_QCt8bHvS"
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"/content/drive/MyDrive/Cambridge/SpamFiltering/df_pre_processing.csv\", sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8eraEekjgDEN",
    "outputId": "858d59c4-c047-494c-ca9a-fba5486f7f11"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df_pre_processing.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25oDgSIiUVC6"
   },
   "source": [
    "### Data formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LgYUqRGxUVC8"
   },
   "outputs": [],
   "source": [
    "# number of attributes retained to build models\n",
    "nb_words = 700"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract lemmas for tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Y2G9-5MQUVC8"
   },
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def sentence_lemma(x):\n",
    "    try:\n",
    "        word_list = nltk.word_tokenize(x)\n",
    "        lemmatized_output = ' '.join([wordnet_lemmatizer.lemmatize(w) for w in word_list])\n",
    "        return lemmatized_output\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ugjk_txoUVC8"
   },
   "outputs": [],
   "source": [
    "df['subject_body'] = df['subject_normal_0'] + df['message_body_normal_0']\n",
    "df['subject_body'] = df['subject_body'].str.lower()\n",
    "df['subject_body'] = df['subject_body'].apply(lambda x: sentence_lemma(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract tokens from subject and body text chunks, compute the information gain of all tokens, and rank tokens according to their information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1pFdh7x9UVC8"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(df.loc[df['subject_body'].notna(),'subject_body'])\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tapILFaYUVC9",
    "outputId": "32a7a17a-e524-40fc-b303-a24d5b8cd1d7"
   },
   "outputs": [],
   "source": [
    "to_add = True\n",
    "q = []  # priority queue\n",
    "\n",
    "# our tokenization method discards punctuation from the token list\n",
    "# as punctuation distribution differs from genuine and spam messages, we manually add them to the list of tokens to consider\n",
    "punctuation = ['\"',\"'\",\"(\",\")\",\"-\",\"+\",\"[\",\"]\",\"{\",\"}\",\";\",\":\",\",\",\"\\\",\",\"<\",\">\",\".\",\"/\",\"?\",\"@\",\"#\",\"$\",\"%\",\"^\",\"&\",\"*\",\"_\",\"~\"]\n",
    "token_list = vectorizer.get_feature_names() + punctuation\n",
    "\n",
    "text_all = ' '.join(df.loc[df['subject_body'].notna(),'subject_body'])\n",
    "\n",
    "for token in token_list:\n",
    "    to_add = True\n",
    "    # we only consider tokens that appear at least in 4 messages, and that have less than 15 characters\n",
    "    if len(token) > 15 or text_all.count(token) <= 4:\n",
    "        to_add = False\n",
    "        continue\n",
    "    if to_add:\n",
    "        try:\n",
    "            # we add tokens with their information gain to the priority queue\n",
    "            heapq.heappush(q, (info_gain.info_gain(df.spam, df.subject_body.str.contains(token)), token))\n",
    "        except:\n",
    "            continue\n",
    "heapq.nlargest(10, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "700 tokens with best information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_best_ig = ['click',\n",
    " 'lick',\n",
    " 'clic',\n",
    " 'cli',\n",
    " 'lic',\n",
    " 'wrote',\n",
    " 'ick',\n",
    " 'cl',\n",
    " 'bsc',\n",
    " 'subscrib',\n",
    " 'ubscribe',\n",
    " 'subscribe',\n",
    " 'cribe',\n",
    " 'ribe',\n",
    " 'remov',\n",
    " 'scribe',\n",
    " 'unsub',\n",
    " 'scr',\n",
    " 'offer',\n",
    " 'cr',\n",
    " 'unsubs',\n",
    " 'remove',\n",
    " 'emove',\n",
    " 'unsubscr',\n",
    " 'unsubscri',\n",
    " 'unsubscrib',\n",
    " 'nsubscribe',\n",
    " 'unsubscribe',\n",
    " 'unsu',\n",
    " 'cri',\n",
    " 'mov',\n",
    " 'receiv',\n",
    " 'recei',\n",
    " 'free',\n",
    " 'offe',\n",
    " 'ck',\n",
    " 'site',\n",
    " 'move',\n",
    " 'hope',\n",
    " 'wr',\n",
    " 'fre',\n",
    " 'rot',\n",
    " 'receive',\n",
    " 'eceive',\n",
    " 'sub',\n",
    " 'rib',\n",
    " 'web',\n",
    " 'eb',\n",
    " 'ffer',\n",
    " 'bsit',\n",
    " 'website',\n",
    " 'ebsite',\n",
    " 'websi',\n",
    " 'bsi',\n",
    " 'hop',\n",
    " 'your',\n",
    " 'li',\n",
    " 'edi',\n",
    " 'our',\n",
    " 'college',\n",
    " 'rday',\n",
    " 'rec',\n",
    " 'iday',\n",
    " 'think',\n",
    " 'hink',\n",
    " 'rem',\n",
    " 'oved',\n",
    " 'but',\n",
    " 'mailing',\n",
    " 'ub',\n",
    " 'receiving',\n",
    " 'guaran',\n",
    " 'guarantee',\n",
    " 'ite',\n",
    " 'wel',\n",
    " 'ailing',\n",
    " 'oing',\n",
    " 'fer',\n",
    " 'product',\n",
    " 'going',\n",
    " 'goin',\n",
    " 'morrow',\n",
    " 'tomorro',\n",
    " 'tomorrow',\n",
    " 'orrow',\n",
    " 'mark',\n",
    " 'sit',\n",
    " 'good',\n",
    " 'valu',\n",
    " 'cre',\n",
    " 'well',\n",
    " 'evenin',\n",
    " 'sc',\n",
    " 'universit',\n",
    " 'evening',\n",
    " 'university',\n",
    " 'off',\n",
    " 'emoved',\n",
    " 'my',\n",
    " 'goo',\n",
    " 'sorry',\n",
    " 'duct',\n",
    " 'low',\n",
    " 'prod',\n",
    " 'love',\n",
    " 'lin',\n",
    " 'lov',\n",
    " 'ov',\n",
    " 'thanks',\n",
    " 'anyway',\n",
    " 'affiliate',\n",
    " 'iz',\n",
    " 'know',\n",
    " 'kno',\n",
    " 'sav',\n",
    " 'coll',\n",
    " 'here',\n",
    " 'ib',\n",
    " 'opt',\n",
    " 'online',\n",
    " 'va',\n",
    " 'nive',\n",
    " 'onli',\n",
    " 'onlin',\n",
    " 'stud',\n",
    " 'ei',\n",
    " 'removed',\n",
    " 'oon',\n",
    " 'credi',\n",
    " 'rod',\n",
    " 'moved',\n",
    " 'ote',\n",
    " 'ved',\n",
    " 'nl',\n",
    " 'quite',\n",
    " 'dear',\n",
    " 'fe',\n",
    " 'fr',\n",
    " 'thank',\n",
    " 'rant',\n",
    " 'mo',\n",
    " 'ze',\n",
    " 'though',\n",
    " 'oin',\n",
    " 'link',\n",
    " 'christ',\n",
    " 'saturday',\n",
    " 'prove',\n",
    " 'edit',\n",
    " 'sd',\n",
    " 'tues',\n",
    " 'tuesday',\n",
    " 'some',\n",
    " 'nda',\n",
    " 'thin',\n",
    " 'sex',\n",
    " 'som',\n",
    " 'friday',\n",
    " 'orry',\n",
    " 'guaranteed',\n",
    " 'riday',\n",
    " 'ic',\n",
    " 'shipp',\n",
    " 'below',\n",
    " 'ante',\n",
    " 'credit',\n",
    " 'rda',\n",
    " 'monday',\n",
    " 'onday',\n",
    " 'dit',\n",
    " 'save',\n",
    " 'go',\n",
    " 'acy',\n",
    " 'lua',\n",
    " 'list',\n",
    " 'student',\n",
    " 'iv',\n",
    " 'riber',\n",
    " 'dollar',\n",
    " 'medic',\n",
    " 'subscriber',\n",
    " 'meeting',\n",
    " 'usine',\n",
    " 'busines',\n",
    " 'usiness',\n",
    " 'business',\n",
    " 'siness',\n",
    " 'cial',\n",
    " 'there',\n",
    " 'thur',\n",
    " 'partner',\n",
    " 'request',\n",
    " 'market',\n",
    " 'servic',\n",
    " 'thurs',\n",
    " 'ff',\n",
    " 'service',\n",
    " 'servi',\n",
    " 'weekend',\n",
    " 'rida',\n",
    " 'opted',\n",
    " 'pray',\n",
    " 'sunday',\n",
    " 'week',\n",
    " 'pra',\n",
    " 'special',\n",
    " 'pecial',\n",
    " 'privacy',\n",
    " 'si',\n",
    " 'doll',\n",
    " 'hin',\n",
    " 'req',\n",
    " 'ur',\n",
    " 'incr',\n",
    " 'future',\n",
    " 'futur',\n",
    " 'yesterday',\n",
    " 'altho',\n",
    " 'money',\n",
    " 'ive',\n",
    " 'thursday',\n",
    " 'tue',\n",
    " 'limi',\n",
    " 'ncr',\n",
    " 'serv',\n",
    " 'holiday',\n",
    " 'wee',\n",
    " 'althoug',\n",
    " 'fut',\n",
    " 'although',\n",
    " 'oney',\n",
    " 'shipping',\n",
    " 'valuable',\n",
    " 'day',\n",
    " 'morn',\n",
    " 'lecture',\n",
    " 'bel',\n",
    " 'limited',\n",
    " 'million',\n",
    " 'millio',\n",
    " 'mited',\n",
    " 'ere',\n",
    " 'rtn',\n",
    " 'limite',\n",
    " 'morning',\n",
    " 'lion',\n",
    " 'noon',\n",
    " 'name',\n",
    " 'red',\n",
    " 'fast',\n",
    " 'xu',\n",
    " 'hri',\n",
    " 'rtg',\n",
    " 'soo',\n",
    " 'ser',\n",
    " 'marketing',\n",
    " 'lowest',\n",
    " 'noo',\n",
    " 'did',\n",
    " 'so',\n",
    " 'chee',\n",
    " 'siz',\n",
    " 'soon',\n",
    " 'pt',\n",
    " 'which',\n",
    " 'smiley',\n",
    " 'ley',\n",
    " 'hich',\n",
    " 'seem',\n",
    " 'mortgage',\n",
    " ':',\n",
    " 'thu',\n",
    " 'mill',\n",
    " 'owes',\n",
    " 'hopeful',\n",
    " 'tn',\n",
    " 'ipp',\n",
    " 'increase',\n",
    " 'limit',\n",
    " 'hall',\n",
    " 'mg',\n",
    " 'sol',\n",
    " 'hav',\n",
    " 'ot',\n",
    " 'am',\n",
    " 'hopefully',\n",
    " 'rwa',\n",
    " 'ney',\n",
    " 'had',\n",
    " 'this',\n",
    " 'lio',\n",
    " 'ia',\n",
    " 'smile',\n",
    " 'afternoon',\n",
    " 'dol',\n",
    " 'inc',\n",
    " 'approved',\n",
    " 'cheer',\n",
    " 'hat',\n",
    " 'pay',\n",
    " 'prayer',\n",
    " 'simply',\n",
    " 'christmas',\n",
    " 'tha',\n",
    " 'proved',\n",
    " 'eek',\n",
    " 'ara',\n",
    " 'uld',\n",
    " 'lot',\n",
    " 'vice',\n",
    " 'imply',\n",
    " 'after',\n",
    " 'fter',\n",
    " 'wednesday',\n",
    " 'exual',\n",
    " 'have',\n",
    " 'sexual',\n",
    " 'approve',\n",
    " 'ning',\n",
    " 'age',\n",
    " 'cia',\n",
    " 'quit',\n",
    " 'thing',\n",
    " 'upply',\n",
    " 'west',\n",
    " 'lu',\n",
    " 'meet',\n",
    " 'oval',\n",
    " 'priv',\n",
    " 'postal',\n",
    " 'supply',\n",
    " 'speak',\n",
    " 'aft',\n",
    " 'asy',\n",
    " 'talk',\n",
    " 'ther',\n",
    " 'line',\n",
    " 'him',\n",
    " 'ray',\n",
    " 'eq',\n",
    " 'ip',\n",
    " 'ec',\n",
    " 'du',\n",
    " 'ym',\n",
    " 'ably',\n",
    " 'ship',\n",
    " 'tee',\n",
    " 'gt',\n",
    " 'lovely',\n",
    " 'thousand',\n",
    " 'ial',\n",
    " ',',\n",
    " 'na',\n",
    " 'erc',\n",
    " 'doctor',\n",
    " 'gua',\n",
    " 'organis',\n",
    " 'next',\n",
    " 'sun',\n",
    " 'octor',\n",
    " 'lunch',\n",
    " 'dinner',\n",
    " 'shoul',\n",
    " 'qual',\n",
    " 'probabl',\n",
    " 'purchase',\n",
    " 'mile',\n",
    " 'should',\n",
    " 'her',\n",
    " 'his',\n",
    " 'spec',\n",
    " 'bc',\n",
    " 'rv',\n",
    " 'probably',\n",
    " 'nin',\n",
    " 'ni',\n",
    " 'forward',\n",
    " 'month',\n",
    " 'nice',\n",
    " 'might',\n",
    " 'tg',\n",
    " 'perhaps',\n",
    " 'onth',\n",
    " 'iva',\n",
    " 'went',\n",
    " 'room',\n",
    " 'ce',\n",
    " 'sim',\n",
    " 'hip',\n",
    " 'size',\n",
    " 'fte',\n",
    " 'color',\n",
    " 'oo',\n",
    " 'aff',\n",
    " 'church',\n",
    " 'duat',\n",
    " 'col',\n",
    " 'ken',\n",
    " 'hey',\n",
    " 'wed',\n",
    " 'easy',\n",
    " '00',\n",
    " 'ase',\n",
    " 'marked',\n",
    " 'href',\n",
    " 'agin',\n",
    " 'vac',\n",
    " 'ply',\n",
    " 'posta',\n",
    " 'ua',\n",
    " 'say',\n",
    " 'trade',\n",
    " 'chase',\n",
    " 'lender',\n",
    " 'custo',\n",
    " 'rom',\n",
    " 'yet',\n",
    " 'graduate',\n",
    " 'customer',\n",
    " 'removal',\n",
    " 'oup',\n",
    " 'su',\n",
    " 'ail',\n",
    " 'pg',\n",
    " 'movie',\n",
    " 'peak',\n",
    " 'hee',\n",
    " 'dv',\n",
    " 'custom',\n",
    " 'vi',\n",
    " 'solicit',\n",
    " 'value',\n",
    " 'cust',\n",
    " 'suppl',\n",
    " 'yy',\n",
    " 'uality',\n",
    " 'that',\n",
    " 'birthday',\n",
    " 'saving',\n",
    " 'sf',\n",
    " 'oi',\n",
    " 'trademark',\n",
    " 'qualit',\n",
    " 'rather',\n",
    " 'arg',\n",
    " 'rma',\n",
    " 'quality',\n",
    " 'scription',\n",
    " 'price',\n",
    " 'rade',\n",
    " 'chas',\n",
    " 'over',\n",
    " 'pro',\n",
    " 'retail',\n",
    " 'nex',\n",
    " 'linguist',\n",
    " 'high',\n",
    " 'hig',\n",
    " 'safe',\n",
    " 'ay',\n",
    " 'hundred',\n",
    " 'rid',\n",
    " 'reall',\n",
    " 'usa',\n",
    " 'really',\n",
    " 'hing',\n",
    " 'ag',\n",
    " 'more',\n",
    " 'dieti',\n",
    " 'eight',\n",
    " 'faster',\n",
    " 'dieting',\n",
    " 'ger',\n",
    " 'finish',\n",
    " 'finis',\n",
    " 'from',\n",
    " 'bout',\n",
    " 'eas',\n",
    " 'net',\n",
    " 'hear',\n",
    " 'study',\n",
    " 'nte',\n",
    " 'rice',\n",
    " 'about',\n",
    " 'bz',\n",
    " 'yme',\n",
    " 'zp',\n",
    " 'ease',\n",
    " 'linguistic',\n",
    " 'pri',\n",
    " 'till',\n",
    " 'hg',\n",
    " 'mat',\n",
    " 'sometime',\n",
    " 'reserved',\n",
    " 'video',\n",
    " 'yon',\n",
    " 'unsol',\n",
    " 'solicite',\n",
    " 'subscribed',\n",
    " 'bscribed',\n",
    " 'seein',\n",
    " 'ici',\n",
    " 'solicited',\n",
    " 'sound',\n",
    " 'seeing',\n",
    " 'unsolicited',\n",
    " 'weight',\n",
    " 'urself',\n",
    " 'seems',\n",
    " 'gues',\n",
    " 'redu',\n",
    " 'sand',\n",
    " 'ourself',\n",
    " 'fro',\n",
    " 'language',\n",
    " 'nger',\n",
    " 'zi',\n",
    " 'gag',\n",
    " 'gage',\n",
    " 'mp',\n",
    " 'got',\n",
    " 'yourself',\n",
    " 'me',\n",
    " 'guess',\n",
    " 'flat',\n",
    " 'athe',\n",
    " 'obligation',\n",
    " '30pm',\n",
    " 'langu',\n",
    " 'payment',\n",
    " 'ant',\n",
    " 'uess',\n",
    " 'fit',\n",
    " 'nso',\n",
    " 'then',\n",
    " 'earn',\n",
    " 'rever',\n",
    " 'fwd',\n",
    " 'ideo',\n",
    " 'umm',\n",
    " '100',\n",
    " 'people',\n",
    " 'motion',\n",
    " 'she',\n",
    " 'edici',\n",
    " 'dicine',\n",
    " 'they',\n",
    " 'edicine',\n",
    " 'uage',\n",
    " 'deo',\n",
    " 'medicine',\n",
    " 'emark',\n",
    " 'emar',\n",
    " 'exclusive',\n",
    " 'loan',\n",
    " 'kt',\n",
    " 'message',\n",
    " 'vid',\n",
    " 'messag',\n",
    " 'doc',\n",
    " '30p',\n",
    " 'revers',\n",
    " 'formal',\n",
    " 'ture',\n",
    " 'instruction',\n",
    " 'ont',\n",
    " 'nth',\n",
    " 'weigh',\n",
    " 'also',\n",
    " 'informatio',\n",
    " 'information',\n",
    " 'lend',\n",
    " 'nformation',\n",
    " 'ness',\n",
    " 'muscle',\n",
    " 'avenue',\n",
    " 'reserve',\n",
    " 'income',\n",
    " 'erect',\n",
    " 'tv',\n",
    " 'ek',\n",
    " 'discreet',\n",
    " 'hl',\n",
    " 'enter',\n",
    " 'see',\n",
    " 'yd',\n",
    " 'ality',\n",
    " 'hda',\n",
    " 'instruct',\n",
    " 'qr',\n",
    " 'proven',\n",
    " 'gation',\n",
    " 'exclu',\n",
    " 'christian',\n",
    " 'wish',\n",
    " 'instruc',\n",
    " 'sage',\n",
    " 'isf',\n",
    " 'informat',\n",
    " 'ende',\n",
    " 'would',\n",
    " 'formation',\n",
    " 'ello',\n",
    " 'guy',\n",
    " 'yz',\n",
    " 'through',\n",
    " 'dad',\n",
    " 'meal',\n",
    " 'fb',\n",
    " 'imp',\n",
    " 'improve',\n",
    " 'throug',\n",
    " 'improv',\n",
    " 'ich',\n",
    " 'rmation',\n",
    " 'liga',\n",
    " 'git',\n",
    " 'nigh',\n",
    " 'mar',\n",
    " 'far',\n",
    " 'ngt',\n",
    " 'hic',\n",
    " 'pill',\n",
    " 'night',\n",
    " 'course',\n",
    " 'informa',\n",
    " 'investment',\n",
    " 'cur',\n",
    " 'ques',\n",
    " 'instru',\n",
    " 'nlarge',\n",
    " 'enlarge',\n",
    " 'ling',\n",
    " 'deliver',\n",
    " 'mati',\n",
    " '%',\n",
    " 'ear',\n",
    " 'scribed',\n",
    " 'risk',\n",
    " 'geo',\n",
    " 'okay',\n",
    " 'nlar',\n",
    " 'sag',\n",
    " 'bh',\n",
    " 'linguistics',\n",
    " 'mation',\n",
    " 'uc',\n",
    " 'satis',\n",
    " 'instant',\n",
    " 'recurring',\n",
    " 'enlarg',\n",
    " 'enlar',\n",
    " 'lud',\n",
    " 'prof',\n",
    " 'wonder',\n",
    " 'prob',\n",
    " 'tio',\n",
    " 'subscription',\n",
    " 'uite',\n",
    " 'clud',\n",
    " 'practical',\n",
    " 'quest',\n",
    " 'tion',\n",
    " 'liver',\n",
    " 'exerci',\n",
    " 'resting',\n",
    " 'fri',\n",
    " 'fini',\n",
    " 'med',\n",
    " '34',\n",
    " 'busy',\n",
    " 'interesti',\n",
    " 'icin',\n",
    " 'ave',\n",
    " 'vie',\n",
    " 'stuff',\n",
    " 'hello',\n",
    " 'usc',\n",
    " 'disco',\n",
    " 'interestin',\n",
    " 'coul',\n",
    " 'sletter',\n",
    " 'afe',\n",
    " 'gi',\n",
    " 'reduce',\n",
    " 'interesting',\n",
    " 'could',\n",
    " 'consumer',\n",
    " 'rough',\n",
    " 'rof',\n",
    " 'thro',\n",
    " 'medical',\n",
    " 'exercise',\n",
    " 'thr',\n",
    " 'ran',\n",
    " 'agra',\n",
    " 'diet',\n",
    " 'usi',\n",
    " 'iu']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of the tokens retained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iIctBwkiUVC9",
    "outputId": "4a122adb-f621-4adb-df6f-de7ab03bf93c"
   },
   "outputs": [],
   "source": [
    "vocab_best_ig = pd.DataFrame(heapq.nlargest(nb_words,q))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names_tf = [col + '_tf' for col in vocab_best_ig]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe with Term-Frequency attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of occurrences of the tokens retained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf = df.copy()\n",
    "for token in vocab_best_ig:\n",
    "    df_tf[token + '_tf'] = df_tf.subject_body.str.count(token)\n",
    "df_tf[col_names_tf] = df_tf[col_names_tf].fillna(0)\n",
    "df_tf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stored best tokens count in a dataframe for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_train = df_tf[list(col_names_tf) + ['spam']].dropna()\n",
    "df_tf_train = df_tf_train.astype(int)\n",
    "df_tf_train.to_csv(\"data_tf/data_\" + str(nb_words) + \".train\", sep=',', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe with Boolean attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boolean attributes indicating if a given message contains a given token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "id": "TYAzZn-mUVC9",
    "outputId": "fe901de2-2c27-40d2-c110-d3c236aa2e3c"
   },
   "outputs": [],
   "source": [
    "df_timbl = df.copy()\n",
    "for token in vocab_best_ig:\n",
    "    df_timbl[token] = df_timbl.subject_body.str.contains(token)\n",
    "df_timbl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stored best boolean attributes in a dataframe for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJdwAfy7UVC9"
   },
   "outputs": [],
   "source": [
    "df_timbl_train = df_timbl[list(vocab_best_ig) + ['spam']].dropna()\n",
    "df_timbl_train = df_timbl_train.astype(int)\n",
    "df_timbl_train.to_csv(\"data_timbl/data_\" + str(nb_words) + \".train\", sep=',', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTZF4Gw6UVC_"
   },
   "source": [
    "## Adaptation set pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load and extract data from the adaptation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PamDsu76UVC_"
   },
   "outputs": [],
   "source": [
    "df_gen_adap = xml_to_df(\"GenSpam/adapt_GEN.ems\")\n",
    "df_gen_adap['spam'] = False\n",
    "df_spam_adap = xml_to_df(\"GenSpam/adapt_SPAM.ems\")\n",
    "df_spam_adap['spam'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ycoryZQcUVC_"
   },
   "outputs": [],
   "source": [
    "df_adap = df_gen_adap.append(df_spam_adap)\n",
    "df_adap = df_adap.sample(frac = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2PQQDwLUVDA"
   },
   "outputs": [],
   "source": [
    "df_adap = df_adap.reset_index().drop(['index'], axis=1)\n",
    "df_adap.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clean and format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1zoPMk-UVDA"
   },
   "outputs": [],
   "source": [
    "filter_col = [col for col in df_adap if col.startswith('message_body') or col.startswith('subject_normal')]\n",
    "for col in filter_col:\n",
    "    df_adap[col] = df_adap[col].apply(lambda x: str(x).replace('\\n','').replace('\\r','').replace('^',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYVa2RYqUVDA"
   },
   "outputs": [],
   "source": [
    "df_adap['subject_body'] = df_adap['subject_normal_0'] + df_adap['message_body_normal_0']\n",
    "df_adap['subject_body'] = df_adap['subject_body'].str.lower()\n",
    "df_adap['subject_body'] = df_adap['subject_body'].apply(lambda x: sentence_lemma(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract Term-Frequency attributes for the best tokens retained and save the corresponding dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_adap = df_adap.copy()\n",
    "for token in vocab_best_ig:\n",
    "    df_tf_adap[token + '_tf'] = df_tf_adap.subject_body.str.count(token)\n",
    "df_tf_adap[col_names_tf] = df_tf_adap[col_names_tf].fillna(0)\n",
    "df_tf_adap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_adap = df_tf_adap[list(col_names_tf) + ['spam']].dropna()\n",
    "df_tf_adap = df_tf_adap.astype(int)\n",
    "df_tf_adap.to_csv(\"data_tf/data_\" + str(nb_words) + \".adap\", sep=',', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract boolean attributes for the best tokens retained and save the corresponding dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qq6OgY7eUVDA"
   },
   "outputs": [],
   "source": [
    "df_timbl_adap = df_adap.copy()\n",
    "for token in vocab_best_ig:\n",
    "    df_timbl_adap[token] = df_timbl_adap.subject_body.str.contains(token)\n",
    "df_timbl_adap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TaQNwAnXUVDA"
   },
   "outputs": [],
   "source": [
    "df_timbl_adap = df_timbl_adap[list(vocab_best_ig) + ['spam']]\n",
    "df_timbl_adap = df_timbl_adap.astype(int)\n",
    "df_timbl_adap.to_csv(\"data_timbl/data_\" + str(nb_words) + \".adap\", sep=',', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTZF4Gw6UVC_"
   },
   "source": [
    "## Test set pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load and extract data from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PamDsu76UVC_"
   },
   "outputs": [],
   "source": [
    "df_gen_test = xml_to_df(\"GenSpam/test_GEN.ems\")\n",
    "df_gen_test['spam'] = False\n",
    "df_spam_test = xml_to_df(\"GenSpam/test_SPAM.ems\")\n",
    "df_spam_test['spam'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ycoryZQcUVC_"
   },
   "outputs": [],
   "source": [
    "df_test = df_gen_test.append(df_spam_test)\n",
    "df_test = df_test.sample(frac = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2PQQDwLUVDA"
   },
   "outputs": [],
   "source": [
    "df_test = df_test.reset_index().drop(['index'], axis=1)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clean and format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1zoPMk-UVDA"
   },
   "outputs": [],
   "source": [
    "filter_col = [col for col in df_test if col.startswith('message_body') or col.startswith('subject_normal')]\n",
    "for col in filter_col:\n",
    "    df_test[col] = df_test[col].apply(lambda x: str(x).replace('\\n','').replace('\\r','').replace('^',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYVa2RYqUVDA"
   },
   "outputs": [],
   "source": [
    "df_test['subject_body'] = df_test['subject_normal_0'] + df_test['message_body_normal_0']\n",
    "df_test['subject_body'] = df_test['subject_body'].str.lower()\n",
    "df_test['subject_body'] = df_test['subject_body'].apply(lambda x: sentence_lemma(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract Term-Frequency attributes for the best tokens retained and save the corresponding dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_test = df_test.copy()\n",
    "for token in vocab_best_ig:\n",
    "    df_tf_test[token + '_tf'] = df_tf_test.subject_body.str.count(token)\n",
    "df_tf_test[col_names_tf] = df_tf_test[col_names_tf].fillna(0)\n",
    "df_tf_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_test = df_tf_test[list(col_names_tf) + ['spam']].dropna()\n",
    "df_tf_test = df_tf_test.astype(int)\n",
    "df_tf_test.to_csv(\"data_tf/data_\" + str(nb_words) + \".test\", sep=',', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract boolean attributes for the best tokens retained and save the corresponding dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qq6OgY7eUVDA"
   },
   "outputs": [],
   "source": [
    "df_timbl_test = df_test.copy()\n",
    "for token in vocab_best_ig:\n",
    "    df_timbl_test[token] = df_timbl_test.subject_body.str.contains(token)\n",
    "df_timbl_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TaQNwAnXUVDA"
   },
   "outputs": [],
   "source": [
    "df_timbl_test = df_timbl_test[list(vocab_best_ig)[:nb_words] + ['spam']]\n",
    "df_timbl_test = df_timbl_test.astype(int)\n",
    "df_timbl_test.to_csv(\"data_timbl/data_\" + str(nb_words) + \".test\", sep=',', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-WPUMalUVDB"
   },
   "source": [
    "## Naive Bayes models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column names correspond to the tokens with best information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = vocab_best_ig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load all dataframes with Term-Frequency attributes previously created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_train = pd.read_csv('data_tf/data_700.train')\n",
    "df_tf_train.columns = col_names + ['spam']\n",
    "df_tf_adap = pd.read_csv('data_tf/data_700.adap')\n",
    "df_tf_adap.columns = col_names + ['spam']\n",
    "df_tf_test = pd.read_csv('data_tf/data_700.test')\n",
    "df_tf_test.columns = col_names + ['spam']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new dataframe with normalized Term-Frequency attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_train_val = df_tf_train.values\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "df_tf_train_norm = min_max_scaler.fit_transform(df_tf_train_val)\n",
    "df_tf_train_norm = pd.DataFrame(df_tf_train_norm)\n",
    "df_tf_train_norm.columns = col_names + ['spam']\n",
    "\n",
    "df_tf_adap_norm = pd.DataFrame(min_max_scaler.transform(df_tf_adap.values))\n",
    "df_tf_adap_norm.columns = col_names + ['spam']\n",
    "\n",
    "df_tf_test_norm = pd.DataFrame(min_max_scaler.transform(df_tf_test.values))\n",
    "df_tf_test_norm.columns = col_names + ['spam']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load all dataframes with Boolean attributes previously created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timbl_train = pd.read_csv('data_timbl/data_700.train')\n",
    "df_timbl_train.columns = col_names + ['spam']\n",
    "df_timbl_adap = pd.read_csv('data_timbl/data_700.adap')\n",
    "df_timbl_adap.columns = col_names + ['spam']\n",
    "df_timbl_test = pd.read_csv('data_timbl/data_700.test')\n",
    "df_timbl_test.columns = col_names + ['spam']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop to apply different Naive Bayes models to our data, and compute different metrics (precision, recall, F1-score, and AUC) with the train and **adaptation** sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "umc5iYCnUVDB"
   },
   "outputs": [],
   "source": [
    "model_names = [\"Multinomial NB TF\", \"Multinomial NB Boolean\", \"Multivariate Bernoulli NB\", \"Gaussian NB Boolean\", \"Gaussian NB Normalized\"]\n",
    "\n",
    "for model_name in model_names:\n",
    "    # we consider different numbers of attributes\n",
    "    for nb_words in range(50,750,50):\n",
    "        # model and data selection\n",
    "        if model_name == \"Multinomial NB TF\":\n",
    "            model = MultinomialNB()\n",
    "            df_model_train = df_tf_train[col_names[:nb_words] + ['spam']]\n",
    "            df_model_adap = df_tf_adap[col_names[:nb_words] + ['spam']]\n",
    "        elif model_name == \"Multinomial NB Boolean\":\n",
    "            model = MultinomialNB()\n",
    "            df_model_train = df_timbl_train[col_names[:nb_words] + ['spam']]\n",
    "            df_model_adap = df_timbl_adap[col_names[:nb_words] + ['spam']]\n",
    "        elif model_name == \"Multivariate Bernoulli NB\":\n",
    "            model = BernoulliNB()\n",
    "            df_model_train = df_timbl_train[col_names[:nb_words] + ['spam']]\n",
    "            df_model_adap = df_timbl_adap[col_names[:nb_words] + ['spam']]\n",
    "        elif model_name == \"Gaussian NB Boolean\":\n",
    "            model = GaussianNB()\n",
    "            df_model_train = df_timbl_train[col_names[:nb_words] + ['spam']]\n",
    "            df_model_adap = df_timbl_adap[col_names[:nb_words] + ['spam']]\n",
    "        elif model_name == \"Gaussian NB Normalized\":\n",
    "            model = GaussianNB()\n",
    "            df_model_train = df_tf_train_norm[col_names[:nb_words] + ['spam']]\n",
    "            df_model_adap = df_tf_adap_norm[col_names[:nb_words] + ['spam']]\n",
    "\n",
    "        # we create train, adapatation and test sets for models\n",
    "        X_train = df_model_train.drop('spam', axis=1)\n",
    "        Y_train = df_model_train['spam']\n",
    "        X_adap = df_model_adap.drop('spam', axis=1)\n",
    "        Y_adap = df_model_adap['spam']\n",
    "\n",
    "        # we fit models and make predictions\n",
    "        model.fit(X_train, Y_train)\n",
    "        Y_pred = model.predict(X_adap)\n",
    "\n",
    "        # we compute different metrics about predictions\n",
    "        precision = metrics.precision_score(Y_adap, Y_pred)\n",
    "        recall = metrics.recall_score(Y_adap, Y_pred)\n",
    "        f1_score = metrics.f1_score(Y_adap, Y_pred)\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(Y_adap, Y_pred)\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "        # we store results in a dataframe\n",
    "        results = pd.DataFrame([[model_name, nb_words, precision, recall, f1_score, auc]], columns=['model_name', 'nb_words', 'precision', 'recall', 'f1_score', 'auc'])\n",
    "        df_save = pd.read_csv(\"nb_results.csv\")\n",
    "        df_save = df_save.append(results)\n",
    "        df_save.to_csv(\"nb_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop to apply different Naive Bayes models to our data, and compute different metrics (precision, recall, F1-score, and AUC) with the train and **test** sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "umc5iYCnUVDB"
   },
   "outputs": [],
   "source": [
    "model_names = [\"Multinomial NB TF\", \"Multinomial NB Boolean\", \"Multivariate Bernoulli NB\", \"Gaussian NB Boolean\", \"Gaussian NB Normalized\"]\n",
    "\n",
    "for model_name in model_names:\n",
    "    # we consider different numbers of attributes\n",
    "    for nb_words in range(50,750,50):\n",
    "        # model and data selection\n",
    "        if model_name == \"Multinomial NB TF\":\n",
    "            model = MultinomialNB()\n",
    "            df_model_train = df_tf_train[col_names[:nb_words] + ['spam']]\n",
    "            df_model_test = df_tf_test[col_names[:nb_words] + ['spam']]\n",
    "        elif model_name == \"Multinomial NB Boolean\":\n",
    "            model = MultinomialNB()\n",
    "            df_model_train = df_timbl_train[col_names[:nb_words] + ['spam']]\n",
    "            df_model_test = df_timbl_test[col_names[:nb_words] + ['spam']]\n",
    "        elif model_name == \"Multivariate Bernoulli NB\":\n",
    "            model = BernoulliNB()\n",
    "            df_model_train = df_timbl_train[col_names[:nb_words] + ['spam']]\n",
    "            df_model_test = df_timbl_test[col_names[:nb_words] + ['spam']]\n",
    "        elif model_name == \"Gaussian NB Boolean\":\n",
    "            model = GaussianNB()\n",
    "            df_model_train = df_timbl_train[col_names[:nb_words] + ['spam']]\n",
    "            df_model_test = df_timbl_test[col_names[:nb_words] + ['spam']]\n",
    "        elif model_name == \"Gaussian NB Normalized\":\n",
    "            model = GaussianNB()\n",
    "            df_model_train = df_tf_train_norm[col_names[:nb_words] + ['spam']]\n",
    "            df_model_test = df_tf_test_norm[col_names[:nb_words] + ['spam']]\n",
    "\n",
    "        # we create train, adapatation and test sets for models\n",
    "        X_train = df_model_train.drop('spam', axis=1)\n",
    "        Y_train = df_model_train['spam']\n",
    "        X_test = df_model_test.drop('spam', axis=1)\n",
    "        Y_test = df_model_test['spam']\n",
    "\n",
    "        # we fit models and make predictions\n",
    "        model.fit(X_train, Y_train)\n",
    "        Y_pred = model.predict(X_test)\n",
    "\n",
    "        # we compute different metrics about predictions\n",
    "        precision = metrics.precision_score(Y_test, Y_pred)\n",
    "        recall = metrics.recall_score(Y_test, Y_pred)\n",
    "        f1_score = metrics.f1_score(Y_test, Y_pred)\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(Y_test, Y_pred)\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "        # we store results in a dataframe\n",
    "        results = pd.DataFrame([[model_name + \" Test\", nb_words, precision, recall, f1_score, auc]], columns=['model_name', 'nb_words', 'precision', 'recall', 'f1_score', 'auc'])\n",
    "        df_save = pd.read_csv(\"nb_results.csv\")\n",
    "        df_save = df_save.append(results)\n",
    "        df_save.to_csv(\"nb_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "spam_filtering.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
